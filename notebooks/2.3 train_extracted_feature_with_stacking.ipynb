{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7818f1-50da-4a88-be03-76e7c9245377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict, Union\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.models import ResNet152_Weights\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score \n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.special import softmax\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.models.utils import fix_seed\n",
    "from src.data.submission import to_submission\n",
    "from src.data.prepare import (\n",
    "    Create5FoldDataFrame,\n",
    ")\n",
    "from src.data.prepare import load_base_df\n",
    "from src.models.MMBT.dataset import (BokeTextImageDataset, collate_fn)\n",
    "from src.models.MMBT.mmbt import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75729267-928f-47c1-ae58-e0bb7c82054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, submission_df = load_base_df('../dataset/csv/', '../dataset/imgs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef60cf6-1924-4bdb-b8da-6e0f9e528f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_csv_path_list = [\n",
    "    '../dataset/processed/train_has_person.csv',\n",
    "    '../dataset/processed/train_od_counts.csv',\n",
    "    '../dataset/processed/train_text_len.csv',\n",
    "    '../dataset/processed/train_tfidf_vector.csv',\n",
    "    '../dataset/processed/train_similarity.csv'\n",
    "]\n",
    "test_feature_csv_path_list = [\n",
    "    '../dataset/processed/test_has_person.csv',\n",
    "    '../dataset/processed/test_od_counts.csv',\n",
    "    '../dataset/processed/test_text_len.csv',\n",
    "    '../dataset/processed/test_tfidf_vector.csv',\n",
    "    '../dataset/processed/test_similarity.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61121f4-de0e-4085-bc46-d6543ab14413",
   "metadata": {},
   "source": [
    "## foldごとのMMBTの特徴量をlightGBMで学習させた結果とMMBTモデルの予測値で学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebb3a73-85cf-4cf6-8773-fdec1b210b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM用特徴量\n",
    "create_train_valid_test_dict = Create5FoldDataFrame(\n",
    "    '../dataset/processed/5fold_stratified_mmbt_seed_0/',\n",
    "    '../dataset/csv/train.csv',\n",
    "    train_feature_csv_path_list,\n",
    "    test_feature_csv_path_list\n",
    ")\n",
    "train_valid_test_dict = create_train_valid_test_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97add1e9-f1f3-4377-ada0-c2b3da570e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQENCE_LEN = 48\n",
    "\n",
    "\n",
    "class LightGBMInterFace:\n",
    "    def __init__(self, args, stopping_rounds=100):\n",
    "        self.model = LGBMClassifier(**args)\n",
    "        self.stopping_rounds = stopping_rounds\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        self.model.fit(\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            eval_set=(X_valid, y_valid),\n",
    "            callbacks=[early_stopping(stopping_rounds=self.stopping_rounds, verbose=True)],\n",
    "            eval_metric='binary_logloss'\n",
    "        )\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "class LogisticRegressionInterFace:\n",
    "    def __init__(self):\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        self.model.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "def create_valid_feat_with_train_valid_test_dict(train_valid_test_dict, model_list):\n",
    "    assert len(train_valid_test_dict) == len(model_list)\n",
    "    valid_features = []\n",
    "    test_features = []\n",
    "    log_loss_list = []\n",
    "    for i, fold_name in enumerate(train_valid_test_dict):\n",
    "        print(f'fold: {fold_name}')\n",
    "        model = model_list[i]\n",
    "        X_train, X_valid, X_test = (\n",
    "            train_valid_test_dict[fold_name]['train']['X'],\n",
    "            train_valid_test_dict[fold_name]['valid']['X'],\n",
    "            train_valid_test_dict[fold_name]['test']['X']\n",
    "        )\n",
    "        y_train, y_valid = (\n",
    "            train_valid_test_dict[fold_name]['train']['y'],\n",
    "            train_valid_test_dict[fold_name]['valid']['y']\n",
    "        )\n",
    "        model.fit(X_train, y_train, X_valid, y_valid)\n",
    "        y_val_pred = model.predict(X_valid)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        loss = log_loss(train_valid_test_dict[fold_name]['valid']['y'], y_val_pred)\n",
    "        log_loss_list.append(loss)\n",
    "        valid_features.append(y_val_pred)\n",
    "        test_features.append(y_test_pred)\n",
    "    print(f'log loss mean:{np.mean(log_loss_list):.3f}, std:{np.std(log_loss_list):.3f}')\n",
    "    return np.concatenate(valid_features), np.mean(test_features, axis=0), np.mean(log_loss_list)\n",
    "\n",
    "\n",
    "\n",
    "class MMBTInfer:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        preds = []\n",
    "        for ds in tqdm(data_loader):\n",
    "            ds = {k: v.to(self.device) for k, v in ds.items()}\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(**ds).logits\n",
    "                pred = pred.cpu().detach().numpy() if torch.cuda.is_available() else pred.cpu().numpy()\n",
    "                preds.append(softmax(pred, axis=1)[:, 1])\n",
    "        return np.concatenate(preds)\n",
    "\n",
    "\n",
    "def load_pretrained_model(src, device):\n",
    "    model = load_model()\n",
    "    model.load_state_dict(torch.load(src))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_mmbt_valid_feat_and_test_feat(\n",
    "            train_df,\n",
    "            test_df,\n",
    "            log_json_and_model_path_dict,\n",
    "            fold_name_list,\n",
    "            tokenizer=AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\"),\n",
    "            batch_size=32,\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        ):\n",
    "    valid_pred_list = []\n",
    "    test_pred_list = []\n",
    "    log_loss_list = []\n",
    "    test_ds = BokeTextImageDataset(test_df, tokenizer, MAX_SEQENCE_LEN, image_transform=ResNet152_Weights.IMAGENET1K_V2)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "    for fold_name in fold_name_list:\n",
    "        # 1. log jsonファイルからvalidationに用いたindexを取得\n",
    "        with open(log_json_and_model_path_dict[fold_name]['json'], 'r') as f:\n",
    "            valid_idx = json.load(f)['valid_idx']\n",
    "        # 2. 取得したindexを元にtrain_dfからvalidに用いたdfのみのDataLoaderを作成\n",
    "        valid_ds = BokeTextImageDataset(\n",
    "            train_df.iloc[valid_idx],\n",
    "            tokenizer,\n",
    "            MAX_SEQENCE_LEN,\n",
    "            image_transform=ResNet152_Weights.IMAGENET1K_V2\n",
    "        )\n",
    "        valid_dl = DataLoader(valid_ds, batch_size=batch_size, collate_fn=collate_fn, shuffle=False)\n",
    "        # 3. valid DataLoaderを予測 => valid_pred_listに追加(一応, log_lossも計算しておく)\n",
    "        model = load_pretrained_model(log_json_and_model_path_dict[fold_name]['model'], device)\n",
    "        infer = MMBTInfer(model, device)\n",
    "        y_val_pred = infer.predict(valid_dl)\n",
    "        log_loss_list.append(log_loss(train_df.iloc[valid_idx]['is_laugh'], y_val_pred))\n",
    "        # 4. testデータの予測(こちらは、fold_k_submission.csvを直接読み込んでもよい)して、test_pred_listに追加\n",
    "        y_test_pred = infer.predict(test_dl)\n",
    "        valid_pred_list.append(y_val_pred)\n",
    "        test_pred_list.append(y_test_pred)\n",
    "    # 5. test_pred_listの平均を計算(5fold_mean_submission.csvを読み込んでもよい。)\n",
    "    return np.concatenate(valid_pred_list), np.mean(test_pred_list, axis=0), np.mean(log_loss_list)\n",
    "\n",
    "\n",
    "def create_valid_feat_with_1st_feat(model, X_train, y_train, X_test, shuffle_seed=0):\n",
    "    preds = []\n",
    "    val_idxes = []\n",
    "    preds_test = []\n",
    "    log_loss_list = []\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=shuffle_seed)\n",
    "    for (train_idx, valid_idx) in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "        model.fit(X_tr, y_tr, X_val, y_val)\n",
    "        val_pred = model.predict(X_val)\n",
    "        preds.append(val_pred)\n",
    "        val_idxes.append(valid_idx)\n",
    "        log_loss_list.append(log_loss(y_val, val_pred))\n",
    "        test_pred = model.predict(X_test)\n",
    "        preds_test.append(test_pred)\n",
    "    val_idxes = np.concatenate(val_idxes)\n",
    "    preds = np.concatenate(preds)[np.argsort(val_idxes)]\n",
    "    print(f'log loss mean:{np.mean(log_loss_list):.3f}, std:{np.std(log_loss_list):.3f}')\n",
    "    return preds, np.mean(preds_test, axis=0), log_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cc670ac-3120-4d84-9b22-3d3d5af4437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100% 500/500 [01:08<00:00,  7.26it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQENCE_LEN = 48\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = load_pretrained_model(\n",
    "    '../model/5fold_stratified_mmbt_seed_0/fold_2/checkpoint-175/pytorch_model.bin',\n",
    "    device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-whole-word-masking\")\n",
    "test_ds = BokeTextImageDataset(test_df, tokenizer, MAX_SEQENCE_LEN, image_transform=ResNet152_Weights.IMAGENET1K_V2)\n",
    "test_dl = DataLoader(test_ds, batch_size=12, collate_fn=collate_fn, shuffle=False)\n",
    "infer = MMBTInfer(model, device)\n",
    "preds = infer.predict(test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02aed56f-4feb-48db-890e-4407d1810f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: fold_3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[131]\tvalid_0's binary_logloss: 0.638928\n",
      "fold: fold_1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's binary_logloss: 0.644614\n",
      "fold: fold_5\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[129]\tvalid_0's binary_logloss: 0.640103\n",
      "fold: fold_4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[124]\tvalid_0's binary_logloss: 0.643792\n",
      "fold: fold_2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[116]\tvalid_0's binary_logloss: 0.649502\n",
      "log loss mean:0.643, std:0.004\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020599842071533203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 0,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 445021143,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d140cf2d2ebb431699199a047d1ae053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013921976089477539,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 0,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 241669177,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdb068f4d05748739e7baf2b571593b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 156/156 [00:59<00:00,  2.60it/s]\n",
      "100% 188/188 [01:04<00:00,  2.91it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100% 157/157 [00:51<00:00,  3.04it/s]\n",
      "100% 188/188 [00:59<00:00,  3.17it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100% 156/156 [00:51<00:00,  3.03it/s]\n",
      "100% 188/188 [00:59<00:00,  3.14it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100% 156/156 [00:51<00:00,  3.02it/s]\n",
      "100% 188/188 [01:01<00:00,  3.06it/s]\n",
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100% 157/157 [00:53<00:00,  2.94it/s]\n",
      "100% 188/188 [01:04<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss mean:0.642, std:0.001\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "log_json_and_model_path_dict = {\n",
    "    'fold_1': {\n",
    "        'json': '../results/5fold_stratified_mmbt_seed_0/fold_1_log.json',\n",
    "        'model': '../model/5fold_stratified_mmbt_seed_0/fold_1/checkpoint-225/pytorch_model.bin'\n",
    "    },\n",
    "    'fold_2': {\n",
    "        'json': '../results/5fold_stratified_mmbt_seed_0/fold_2_log.json',\n",
    "        'model': '../model/5fold_stratified_mmbt_seed_0/fold_2/checkpoint-175/pytorch_model.bin'\n",
    "    },\n",
    "    'fold_3': {\n",
    "        'json': '../results/5fold_stratified_mmbt_seed_0/fold_3_log.json',\n",
    "        'model': '../model/5fold_stratified_mmbt_seed_0/fold_3/checkpoint-250/pytorch_model.bin'\n",
    "    },\n",
    "    'fold_4': {\n",
    "        'json': '../results/5fold_stratified_mmbt_seed_0/fold_4_log.json',\n",
    "        'model': '../model/5fold_stratified_mmbt_seed_0/fold_4/checkpoint-250/pytorch_model.bin'\n",
    "    },\n",
    "    'fold_5': {\n",
    "        'json': '../results/5fold_stratified_mmbt_seed_0/fold_5_log.json',\n",
    "        'model': '../model/5fold_stratified_mmbt_seed_0/fold_5/checkpoint-175/pytorch_model.bin'\n",
    "    }\n",
    "}\n",
    "lgm_model_list = [\n",
    "    LightGBMInterFace(\n",
    "        {\n",
    "            'num_leaves': 134,\n",
    "            'subsample_freq': 2,\n",
    "            'subsample': 0.5196796955706757,\n",
    "            'colsample_bytree': 0.31998303280144247,\n",
    "            'min_child_samples': 10,\n",
    "            'max_depth': 4,\n",
    "            'reg_alpha': 0.04220057397195014,\n",
    "            'learning_rate': 0.01,\n",
    "            'random_state': SEED,\n",
    "            'n_estimators': 20000,\n",
    "        }\n",
    "    ),\n",
    "    LightGBMInterFace(\n",
    "        {\n",
    "            'num_leaves': 162,\n",
    "            'subsample_freq': 3,\n",
    "            'subsample': 0.8024762586578099,\n",
    "            'colsample_bytree': 0.20644698328203992,\n",
    "            'min_child_samples': 39,\n",
    "            'max_depth': 5,\n",
    "            'reg_alpha': 0.6007249475906198,\n",
    "            'learning_rate': 0.01,\n",
    "            'random_state': SEED,\n",
    "            'n_estimators': 20000,\n",
    "        },\n",
    "    ),\n",
    "    LightGBMInterFace(\n",
    "        {\n",
    "            'num_leaves': 162,\n",
    "            'subsample_freq': 3,\n",
    "            'subsample': 0.8024762586578099,\n",
    "            'colsample_bytree': 0.20644698328203992,\n",
    "            'min_child_samples': 39,\n",
    "            'max_depth': 5,\n",
    "            'reg_alpha': 0.6007249475906198,\n",
    "            'learning_rate': 0.01,\n",
    "            'random_state': SEED,\n",
    "            'n_estimators': 20000,\n",
    "        }\n",
    "    ),\n",
    "    LightGBMInterFace(\n",
    "        {\n",
    "            'num_leaves': 27,\n",
    "            'subsample_freq': 5,\n",
    "            'subsample': 0.3764957033418272,\n",
    "            'colsample_bytree': 0.3047369568241308,\n",
    "            'min_child_samples': 53,\n",
    "            'max_depth': 30,\n",
    "            'reg_alpha': 0.4270897845895936,\n",
    "            'learning_rate': 0.01,\n",
    "            'random_state': SEED,\n",
    "            'n_estimators': 20000,\n",
    "        }\n",
    "    ),\n",
    "    LightGBMInterFace(\n",
    "        {\n",
    "            'num_leaves': 172,\n",
    "            'subsample_freq': 4,\n",
    "            'subsample': 0.3305356794741203,\n",
    "            'colsample_bytree': 0.4523656800127492,\n",
    "            'min_child_samples': 46,\n",
    "            'max_depth': 4,\n",
    "            'reg_alpha': 0.9263461787560893,\n",
    "            'learning_rate': 0.01,\n",
    "            'random_state': SEED,\n",
    "            'n_estimators': 20000,\n",
    "        }\n",
    "    )\n",
    "]\n",
    "lgm_valid_features, lgm_test_features, lgm_log_loss_mean = create_valid_feat_with_train_valid_test_dict(train_valid_test_dict, lgm_model_list)\n",
    "mmbt_valid_features, mmbt_test_features, mmbt_log_loss_mean = create_mmbt_valid_feat_and_test_feat(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    log_json_and_model_path_dict,\n",
    "    list(train_valid_test_dict.keys())\n",
    ")\n",
    "\n",
    "\n",
    "lv2_model = LogisticRegressionInterFace()\n",
    "train_lv1_feat = pd.DataFrame(\n",
    "    {\n",
    "        'mmbt': mmbt_valid_features,\n",
    "        'lgm': lgm_valid_features\n",
    "    }\n",
    ")\n",
    "test_lv1_feat = pd.DataFrame(\n",
    "    {\n",
    "        'mmbt': mmbt_test_features,\n",
    "        'lgm': lgm_test_features\n",
    "    }\n",
    ")\n",
    "y_train = pd.DataFrame(np.concatenate([train_valid_test_dict[fold_name]['valid']['y'] for fold_name in train_valid_test_dict]))\n",
    "_, y_pred, _ = create_valid_feat_with_1st_feat(lv2_model, train_lv1_feat, y_train, test_lv1_feat)\n",
    "to_submission('../dataset/csv/sample_submission.csv', y_pred, '../results/lgm_and_mmbt_stacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aca7a76-1ea8-43b1-9ed9-6e1944ea3526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
